## Search

We implement poor man's parallelization---scripts for splitting the corpus and merging the results back---along with multi-threading to quickly search through a large text corpus. We also provide the option to reduce the amount of searching by reducing the size of the text corpus by preprocessing it --- removing stop words etc. 

There are three scripts --- to be run sequentially --- for the purpose:

1. [Split the text corpus into smaller chunks](#Split-text-corpus-into-smaller-chunks)  
2. [Search](#search)
3. [Merge Search Results](#Merge-Search-Results)

### Installation

To install the dependencies:

```
pip install -r requirements.txt
```

### Split text corpus into smaller chunks

This script splits large text corpora into multiple smaller chunks that can be run on multiple servers.

#### Usage
```
usage: split_text_corpus.py [-h] [-o OUTFILE] [-s SIZE] input

Split large text corpus into smaller chunks

positional arguments:
  input                 CSV input file name

optional arguments:
  -h, --help            show this help message and exit
  -o OUTFILE, --out OUTFILE
                        Output file in CSV (default:
                        chunk_{chunk_id:02d}/{basename}.csv)
  -s SIZE, --size SIZE  Number of row in each chunk (default: 1000)
```

#### Example

```
python split_text_corpus.py -s 1000 tests/text_corpus.csv
```

The script will split [`text_corpus.csv`](text_corpus.csv) into multiple chunk_* directories.

In this case chunk_00, chunk_01, ... chunk_09 directory will be created along with `text_corpus.txt` which will have 1000 rows in it.

The output location and file name convention can be specified by the `-o / --out` command line option. Actually, it is a Python format string where `chunk_id` will replace chunk number starting from 0, and `basename` is input file's name (without path and extension).

### Search for names

This is the script to search names in the text corpus. The input file must contain at least two columns `uniqid` and `text`.

#### Configuration file

The script relies on a configuration file, [`search_names.cfg`](search_names.cfg), [`search_cols.txt`](search_cols.txt) that lists the columns from search file to be included in the output, and [`input_file_cols.txt`](input_file_cols.txt) that lists columns from the file containing the text data to be included in the output.

The configuration file has three sections. In the `[name]` section of the configuration file, there is a variable `file` which you can use to specify a CSV file where `id` and `search` refer to uniqid and keywords to be searched in that file respectively. In this case `id` and `search` are set to `uniqid` and `search_name`, the de-duped output generated by [preprocess](../preprocess/). Section `[editlength]` specifies the minimum string length for that edit distance. `edit1 = 10` means edit distance of 1 is allowed if string longer than 10 characters and `edit2 = 20` means that edit distance of 2 is allowed if the string is longer than 20 characters. We must use the same `editlength` as [`preprocess.cfg`](../preprocess/preprocess.cfg) to avoid getting ambiguous search results. `text` in the `input` section specifies the name of the column that contains the text data to be searched. 

```
[name]
file = ../preprocess/deduped_augmented_clean_names.csv
id = uniqid
search = search_name

[input]
text = text

[editlength]
edit1 = 10
edit2 = 20
```

Once again, if you want to disable `fuzzy' matching, just comment out edit1 and edit2 using a hash sign as follows:

```
# edit1 = 10
# edit2 = 20
```

#### Usage

```
usage: search_names.py [-h] [-c CONFIG] [-m MAX_NAME] [-p PROCESSES]
                       [-o OUTFILE] [--overwritten] [-d] [--clean]
                       input

Search names in text corpus

positional arguments:
  input                 CSV input file name

optional arguments:
  -h, --help            show this help message and exit
  -c CONFIG, --config CONFIG
                        Default configuration file (default: search_names.cfg)
  -m MAX_NAME, --max-name MAX_NAME
                        Maxinum name in search results (default: 20)
  -p PROCESSES, --processes PROCESSES
                        Number of multi-process to run (default: 4)
  -o OUTFILE, --out OUTFILE
                        Search results in CSV (default: search_results.csv)
  --overwritten         Overwritten if output file is exists
  -d, --debug           Enable debug message
  --clean               Clean text column before search
```

#### Example

```
python search_names.py text_corpus.csv
```

By default, the script forks 4 processes (specify by `-p / --processes`) and searches for the names specified by `[name]` section in the configuration file [`search_names.cfg`](search_names.cfg). `-m / --max-name` is used to limit maximum search results. `--overwritten` is used to overwrite the output file if it exists; it is disabled by default. Also `--clean` option is provided to clean the `text` column (remove stop words, special characters etc.) before search. 

The output file (specify by `-o / --out`) will contains all columns from the input file (except `text` column will be replaced by cleaned text if `--clean` is specify) along with the search result columns that are:

    `nameX.uniqid` - uniqid number from name file
    `nameX.n` - occurrences of name found
    `nameX.match` - name found (separated by semi-colon `;` if multiple matches)
    `nameX.start` - start index of name found
    `nameX.end` - end index of name found
    `count` - total occurrences of name found

where `X` is result numbering start from 1 to maximum search results

Please note that row sequence in the output file will not be same as the input file as the script gets results from multi-threaded searching.

### Merge Search Results

Merge search results back from multiple files to a single file.

#### Usage

```
usage: merge_results.py [-h] [-o OUTFILE] [inputs [inputs ...]]

Merge search results from multiple chunks

positional arguments:
  inputs                CSV input file(s) name

optional arguments:
  -h, --help            show this help message and exit
  -o OUTFILE, --out OUTFILE
                        Output file in CSV (default:
                        merged_search_results.csv)
```

#### Example

```
python merge_results.py chunk_00/search_results.csv chunk_01/search_results.csv chunk_02/search_results.csv
```

Above script will merge 3 search results into a single output file. The default is `merged_results.csv`
